---
title: "Supporting relationships"
author: "Rob Challen"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
here::i_am("supporting-relationships.qmd")
library(tidyverse)

logit = function(p) {return(log(p/(1-p)))}
expit = function(x) {return(1/(1+exp(-x)))}

# install.packages(
#   c("odin","odin.dust","dust"),
#   repos = c("https://mrc-ide.r-universe.dev", "https://cloud.r-project.org"))

```

# Supporting rationale

## Signal to noise, sensitivity and specificiy

Viral load is normalised so that a value of 1 is infected and should be
detected at the sensitivity and specificity of the tests. This is to do
with the ease of separating signal from noise. The noise distribution 
upper quantile at a test cut off must be equal to specificity.
at the same test cut off the lower quantile of the signal distribution must
be equal to sensitivity. 

Problem is we don;t know anything about the signal distribution. and it 
is somewhat arbitrary because it is a circular definition, if we impose
a cut-off. This comes down to
how we normalise viral load. 

A cut off of 1 has to be designed that 
noise distribution quantile (spec)% is 1, and 1+noise distribution quantile
is (1-sens)% at 1. If we assume a uniform distributed additive noise 
function we can phrase this in 

$$
E \sim Uniform(min,max) \\
P(E \geq 1) = specificity \\
P(1+E \leq 1) = 1-sensitivity \\
P(E \leq 0) = 1-sensitivity \\
$$

I.e. the (1-sensitivity)% quantile is 0 and the (specificity)% quantile is 1. 
The CDF of this Uniform distribution is a linear function with gradient and 
intercept:

$$

CDF(x) = (specificity + sensitivity - 1) \times x + (1-sensitivity) \\

CDF(min) = 0 \\
(specificity + sensitivity - 1) \times min + (1-sensitivity) = 0 \\
min = \frac{sensitivity-1}{specificity + sensitivity - 1} \\

CDF(max) = 1 \\
(specificity + sensitivity - 1) \times max + (1-sensitivity) = 1 \\
max = \frac{sensitivity}{specificity + sensitivity - 1}

$$

```{r}

sensitivity = 0.8
specificity = 0.95

min = (sensitivity-1)/(specificity + sensitivity - 1) 
max = (sensitivity)/(specificity + sensitivity - 1)

# sens
sum(runif(10000,min,max)+1 > 1)/10000

# spec
sum(runif(10000,min,max) < 1)/10000
```

Easier to transform a 0,1 Uniform using sensitivity and specificity

```{r}

noise = (runif(10000)-(1-sensitivity)) / (sensitivity+specificity-1)

# sens 
sum(1 + noise > 1)/10000

# spec
sum( noise < 1)/10000

```


## Conversions

rate to probability
	
$$
p = 1-e^{-rate}\\
rate = -ln(1-p)
$$

```{r}
p = 0.1 #seq(0,1,0.1)
rate = -log(1-p)

p2 = 1-exp(-rate*c(1,2,4,8,16))
rate2 = -log(1-p2)

rate2/rate

```


$$

period \\
0.5 = p^{period} \\
log(0.5) = period \times log(p) \\
p = e^\frac{log(0.5)}{period}

$$


```{r}
# half life
period = 7
p = exp(log(0.5)/period)
p^7
```

# Scaling probabilities

```{r}


OR = 2
p = seq(0.1,0.9,0.1)

scaled_p = expit(logit(p) + log(OR))

# scaled_p2 = 1-((1-p)^OR)

p/(1-p)
scaled_p/(1-scaled_p)
# scaled_p2/(1-scaled_p2)
```

```{r}
# If we need to reduce the effect of a modifier on the logit scale we can simply
# multiply by the factor. This is equivalent to multiplying the 
# odds by the (odds_ratio ^ factor)

# if this is a number of days then a 1/(1+period) is going to be the factor
# we use 

b = expit(logit(0.01) + seq(-1,1,0.1))

a = expit(logit(0.01) + seq(-1,1,0.1) * 0.5)

```

### Logit normal distributions.

Standard deviation of logit normal distribution is not easy to specify
It would be useful to parametrise using mean and SD. There is a simple relationship
between median and mu. scale parameter not so easy though.

```{r}
median = seq(0.025,0.975,0.05)
scale = 0.1
mu = logit(median);
#sigma = scale*(exp((abs(mu)^(3/4))))

sigma = scale*(2+(abs(mu)^(7/4)))

tmp = bind_rows(lapply(seq_along(mu), \(i) tibble::tibble(
  q = seq(0.01,0.99,0.01),
  x = expit(qnorm(p = seq(0.01,0.99,0.01))*sigma[i]+mu[i]),
  p = (q-lag(q))/(x-lag(x)),
  median = expit(mu[i])          
)))

ggplot(tmp,aes(x=x,y=p,colour=as.factor(median)))+geom_line()

tmp2 = tmp %>% group_by(median) %>%
  mutate(dx=x-lag(x)) %>%
  summarise(
    mean = sum(x*p*dx,na.rm = TRUE),
    sd = sum((x-mean)^2*p*dx,na.rm = TRUE)
  )

ggplot(tmp2,aes(x=median,y=mean))+geom_point()
ggplot(tmp2,aes(x=median,y=sd))+geom_point()
```



## Convex Beta distributions

Mode if alpha and beta are >1 otherwise turns into u shaped distribution

$$
\alpha = (\frac{\mu(1-\mu)}{\sigma^2}-1) \mu \\
\beta = (\frac{\mu(1-\mu)}{\sigma^2}-1) (1-\mu) \\
$$

$$

1 < (\frac{\mu(1-\mu)}{\sigma^2}-1) \mu \\
\frac{1}{\mu}+1 < \frac{\mu(1-\mu)}{\sigma^2} \\
\frac{1+\mu}{\mu} < \frac{\mu(1-\mu)}{\sigma^2} \\
\sigma^2 < \mu^2\frac{1-\mu}{1+\mu} \\

$$

$$

1 < (\frac{\mu(1-\mu)}{\sigma^2}-1) (1-\mu) \\
\frac{1}{1-\mu}+1 < \frac{\mu(1-\mu)}{\sigma^2}  \\
\frac{2-\mu}{1-\mu} < \frac{\mu(1-\mu)}{\sigma^2}  \\
{\sigma^2} < (1-\mu)^2\frac{\mu}{2-\mu} \\
$$



```{r}
rbeta2 = function(
  n, 
  mu = 0.5,
  disp = 0.25
) {

  sigma = sqrt(pmin(
    mu^2*(1-mu)/(1+mu),
    (1-mu)^2*(mu)/(2-mu)
  ))
  
  sd = disp*sigma
  
  # regardless of mu one of alpha or beta should be equal to 1.
  
  alpha = (mu*(1-mu) / sd^2 -1) * mu
  beta = (mu*(1-mu) / sd^2 -1) * (1-mu)
  return(rbeta(n,a,b))
}

mean(rbeta2(1000, 0.5, 0.25))
sd(rbeta2(1000, 0.5, 0.25))
```



# Log likelihood ratio and multiple tests / symptoms

```{r}

tests = tribble(
  ~test, ~sens, ~spec, ~obs_neg, ~delay_fn,
  "PCR", 0.95, 0.99, TRUE, \(t) max(0,(10-t)/10),
  "LFT", 0.8, 0.95, FALSE, \(t) max(0,(5-t)/5),
  "SYMPTOMS", 0.5, 0.8, FALSE, \(t) exp(-t/2)
)

tests = tests %>% mutate(
  log_LR_pos = log(sens / (1-spec)),
  log_LR_neg = log((1-sens) / spec)
)

person = tribble(
  ~time, ~test, ~result,
  0, "PCR", TRUE,
  1, "LFT", TRUE,
  5, "LFT", FALSE,
  0, "SYMPTOMS", TRUE,
  1, "SYMPTOMS", TRUE,
  3, "SYMPTOMS", FALSE
)

person = person %>% inner_join(tests, by="test") %>%
  mutate(
    time_coeff = purrr::map2_dbl(delay_fn,time, ~.x(.y)),
    log_LR = ifelse(result, log_LR_pos, log_LR_neg),
    log_LR2 = ifelse(result, log_LR_pos, log_LR_neg*obs_neg),
    adj_log_LR = time_coeff * log_LR,
    adj_log_LR2 = time_coeff * log_LR2,
  )

prevalence = c(0.001,0.01,0.1)

person %>% crossing(prevalence) %>%
  group_by(prevalence) %>%
  summarise(
    log_LR = sum(log_LR),
    log_LR2 = sum(log_LR2),
    adj_log_LR = sum(adj_log_LR),
    adj_log_LR2 = sum(adj_log_LR2)
  ) %>%
  mutate(
    posterior = expit(logit(prevalence)+log_LR),
    posterior2 = expit(logit(prevalence)+log_LR2),
    adj_posterior = expit(logit(prevalence)+adj_log_LR),
    adj_posterior2 = expit(logit(prevalence)+adj_log_LR2)
  )

```

# Contact matrices



```{r}
partic = readr::read_csv("https://zenodo.org/records/6542524/files/CoMix_uk_participant_common.csv?download=1")
comix = readr::read_csv("https://zenodo.org/records/6542524/files/CoMix_uk_contact_common.csv?download=1")
```

```{r}
partic = partic %>% mutate(
  part_age_min = as.numeric(stringr::str_extract(part_age, "([0-9]+)-([0-9]+)", 1)),
  part_age_max = as.numeric(stringr::str_extract(part_age, "([0-9]+)-([0-9]+)", 2))
)

agediff = comix %>% inner_join(partic, by="part_id") %>% mutate(
  min_age_diff = pmin(part_age_min - cnt_age_est_max, part_age_max - cnt_age_est_min),
  max_age_diff = pmax(part_age_min - cnt_age_est_max, part_age_max - cnt_age_est_min),
  # mid_age_diff = ((part_age_min - cnt_age_est_max)+(part_age_max - cnt_age_est_min))/2
  mid_age_diff = abs((part_age_min+part_age_max)/2 - (cnt_age_est_max+cnt_age_est_min)/2)
) %>% glimpse()

ggplot(agediff)+
  geom_histogram(aes(x=abs(mid_age_diff)), colour="red",binwidth = 1)#+

ggplot(agediff)+
  geom_density(aes(x=abs(mid_age_diff)), colour="red")

  # geom_histogram(aes(x=max_age_diff), colour="blue",binwidth = 5)+
  # geom_histogram(aes(x=(min_age_diff+max_age_diff)/2), colour="magenta",binwidth = 5)
```



```{r}

f_gen = function(a,b,c,d) {
  tmp = function(x) (a+cos(2*pi*(x/c)^d)) / ((a+1)*(b*(x/c)^2+1))
  total = integrate(tmp,0,100)$value
  print(total)
  return(function(x) return(tmp(x)/total))
}

ggplot(agediff)+
  geom_density(aes(x=abs(mid_age_diff)), colour="red")+
  geom_function(fun = f_gen(1.2,0.25,6.25,0.8))+xlim(0,80)


sapply(0:100, function(x) integrate(f_gen(1.2,0.25,6.25,0.8),0,x)$value)

f = f_gen(1.2,0.25,6.25,0.8)

x = tibble(
  agediff = 0:100,
  p = f(0:100)
) %>% mutate(
  rr = p/mean(p)
)

mean(x$p)


```

Probability of 

```{r}
cum = sapply(1:100, function(x) sum(na.omit(agediff$mid_age_diff)<x))

cdf = tibble(
  diff=1:100,
  cdf = cum/max(cum)
)


g_gen = function(a,b,c,d) {
  return(function(x) 1-exp(-x/a)*(1+1/b*sin(2*pi*x^d/c)^2))
}

g = g_gen(20,10,13,0.9)
g(1)

ggplot(cdf,aes(x=diff,y=cdf))+
  geom_line()+
  geom_function(fun = g_gen(19,8,11,0.85), colour="red")+xlim(0,80)
  

```


```{r}

cdf_gen = function(a,b,c,d) {
  return(function(x) a*tan((x+c*(1-x^2)*cos(b*pi/2*x^d))*atan(75/a)))
}

ggplot(cdf,aes(x=cdf,y=diff))+
  geom_line()+
  geom_function(fun = cdf_gen(18,23,0.03,3.5), colour="red")+xlim(0,1)

```




```{r}
library(reshape2)
library(dplyr)
library(ggplot2)
opt=theme(legend.position="none",
          panel.background = element_rect(fill="white"),
          panel.grid=element_blank(),
          axis.ticks=element_blank(),
          axis.title=element_blank(),
          axis.text=element_blank())
hilbert = function(m,n,r) {
  for (i in 1:n)
  {
    tmp=cbind(t(m), m+nrow(m)^2)
    m=rbind(tmp, (2*nrow(m))^r-tmp[nrow(m):1,]+1)
  }
  melt(m) %>% plyr::rename(c("Var1" = "x", "Var2" = "y", "value"="order")) %>% arrange(order)}
# Original
ggplot(hilbert(m=matrix(1), n=1, r=2), aes(x, y)) + geom_path()+ opt
ggplot(hilbert(m=matrix(1), n=2, r=2), aes(x, y)) + geom_path()+ opt
ggplot(hilbert(m=matrix(1), n=3, r=2), aes(x, y)) + geom_path()+ opt
ggplot(hilbert(m=matrix(1), n=4, r=2), aes(x, y)) + geom_path()+ opt
ggplot(hilbert(m=matrix(1), n=5, r=2), aes(x, y)) + geom_path()+ opt
ggplot(hilbert(m=matrix(1), n=6, r=2), aes(x, y)) + geom_path()+ opt
# # Changing order
# ggplot(hilbert(m=matrix(.5), n=5, r=2), aes(x, y)) + geom_path()+ opt
# ggplot(hilbert(m=matrix(0), n=5, r=2), aes(x, y)) + geom_path()+ opt
# ggplot(hilbert(m=matrix(tan(1)), n=5, r=2), aes(x, y)) + geom_path()+ opt
# ggplot(hilbert(m=matrix(3), n=5, r=2), aes(x, y)) + geom_path()+ opt
# ggplot(hilbert(m=matrix(-1), n=5, r=2), aes(x, y)) + geom_path()+ opt
# ggplot(hilbert(m=matrix(log(.1)), n=5, r=2), aes(x, y)) + geom_path()+ opt
# ggplot(hilbert(m=matrix(-15), n=5, r=2), aes(x, y)) + geom_path()+ opt
# ggplot(hilbert(m=matrix(-0.001), n=5, r=2), aes(x, y)) + geom_path()+ opt
# # Polygons
# ggplot(hilbert(m=matrix(log(1)), n=4, r=2), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(.5), n=4, r=2), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(tan(1)), n=5, r=2), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(-15), n=4, r=2), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(-25), n=4, r=2), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(0), n=4, r=2), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(1000000), n=4, r=2), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(-1), n=4, r=2), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(-.00001), n=4, r=2), aes(x, y)) + geom_polygon()+ opt
# # Changing exponent
# gplot(hilbert(m=matrix(log(1)), n=4, r=-1), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(.5), n=4, r=-2), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(tan(1)), n=4, r=6), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(-15), n=3, r=sin(2)), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(-25), n=4, r=-.0001), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(0), n=4, r=200), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(1000000), n=3, r=.5), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(-1), n=4, r=sqrt(2)), aes(x, y)) + geom_polygon()+ opt
# ggplot(hilbert(m=matrix(-.00001), n=4, r=52), aes(x, y)) + geom_polygon()+ opt
# # Polar coordinates
# ggplot(hilbert(m=matrix(1), n=4, r=2), aes(x, y)) + geom_polygon()+ coord_polar()+opt
# ggplot(hilbert(m=matrix(-1), n=5, r=2), aes(x, y)) + geom_polygon()+ coord_polar()+opt
# ggplot(hilbert(m=matrix(.1), n=2, r=.5), aes(x, y)) + geom_polygon()+ coord_polar()+opt
# ggplot(hilbert(m=matrix(1000000), n=2, r=.1), aes(x, y)) + geom_polygon()+ coord_polar()+opt
# ggplot(hilbert(m=matrix(.25), n=3, r=3), aes(x, y)) + geom_polygon()+ coord_polar()+opt
# ggplot(hilbert(m=matrix(tan(1)), n=5, r=1), aes(x, y)) + geom_polygon()+ coord_polar()+opt
# ggplot(hilbert(m=matrix(1), n=4, r=1), aes(x, y)) + geom_polygon()+ coord_polar()+opt
# ggplot(hilbert(m=matrix(log(1)), n=3, r=sin(2)), aes(x, y)) + geom_polygon()+ coord_polar()+opt
# ggplot(hilbert(m=matrix(-.0001), n=4, r=25), aes(x, y)) + geom_polygon()+ coord_polar()+opt

```

# Scaling an Odds ratio by a probability

Suppose we have some risk as an odds ratio.

(Deprecated: this was avoided by not scaling the odds)

We have the probability of contact defined as a baseline probability multiplied
by a mobility odds ratio. Scaling takes the form of the expit of the 
logit of probability plus by the log odds:

```{r}
baseline_p_contact = 0.6
mobilityOR = seq(0.5,1.50,0.1)

modified_p_contact = expit(logit(baseline_p_contact)+log(mobilityOR))
modified_p_contact
```

We are considering the case where compliance to instruction to self-isolate is 
variable. If fully compliant to instruction a contact will not happen

The problem is that sometimes a low probability of compliance to instruction 
means that the resulting probability of contact will be higher. We would like 
to reflect this in the odds ratio rather than in the joint probability.

The adjusted final probability of contact is $P(contact) \cup Â¬(P(compliance))$

contact | compliant | result
------- | --------- | ------
YES     | YES       | NO
YES     | NO        | YES
NO      | YES       | NO
NO      | NO        | NO


```{r}

p_compliance = 0.8
p_adj = modified_p_contact * (1-p_compliance)
p_adj

```

In this case the mobility odds ratio 
will increase typically from a level less than 1 towards 1. In some situations
mobility OR can be > 1 though. Poor compliance in this situation could mean extra
mobility. A compliance level of 0.9 will result in less of an upwards adjustment 
to the mobility odds than a compliance of 0.1. A compliance of zero doesn't make 
specific sense. A compliance of 1 means that the mobility odd shouldn't change.

# Testing and risk

We want to identify the risk to a contact made at a particular time after a 
contact is made. This depends on symptoms and test results.

```
contact :                      â”Œâ”€ 4 â”€â”€â”€ 5 â”€â”€â”€ 6 â”€â”€â”€ 7 â”€â”€â”€ 8 â”€â”€â”€ 9 
day     : 0 â”€â”€â”€ 1 â”€â”€â”€ 2 â”€â”¬â”€ 3 â”€â”´â”€ 4 â”€â”€â”€ 5 â”€â”¬â”€ 6 â”€â”€â”€ 7 â”€â”€â”€ 8 â”€â”€â”€ 9 
test    :                â”‚                 â””â”€ 6 â”€â”€â”€ 7 â”€â”¬â”€ 8 â”€â”€â”€ 9 
result  :                â”‚                             â””â”€ 8 â”€â”€â”€ 9 
symptoms:                â””â”€ 3 â”€â”€â”€ 4 â”€â”€â”€ 5  
â”¬ â”´ â”œ â”€ â”œ â”” â”‚
```





```
contact :                      â”Œâ”€ 4 â”€â”€â”€ 5 â”€â”€â”€ 6 â”€â”€â”€ 7 â”€â”€â”€ 8 â”€â”€â”€ 9 
day     : 0 â”€â”€â”€ 1 â”€â”€â”€ 2 â”€â”¬â”€ 3 â”€â”´â”€ 4 â”€â”€â”€ 5 â”€â”¬â”€ 6 â”€â”€â”€ 7 â”€â”€â”€ 8 â”€â”¬â”€ 9 
symptoms:                A                 B                 C
```
Symptom A is relevant and known at the point of contact. 
Symptom B is relevant because it is within the incubation period but only known
after day 5. Symptom C is not very relevant, although the probability of being
infectious on day 3. 
Symptom B is less relevant than symptom A because the probability of being 
infectious is lower?


From a person history I can find symptoms that are relevant to that point in time
looking forward and backwards, but this will change over time as might include
stuff that hasn't happened yet.

From a person history I can find tests that are relevant to that point in time
looking forward and backwards.

Its only worth doing if there was a contact on that day and that contact is evaluating its 
risk. This is probable as most days there will be some contacts.

So I can evaluate what todays symptoms means for people in the past or future
but this is trivial and pre-calculation may not bring benefit.

We need to know for a contact (i.e. person history) what that will be in the
future.

Maybe I need to ask the question if the risk of contacting me is evaluated today (Person state)
what is it given the date of contact? For each individual this is an array of
the symptoms and tests as known today. back as long as 1 infectious period, but
that might be relevant to 2 infectious periods.

There is no point storing this outside of each state as it will likely change.
memory is not an issue. 

direct evidence could be nothing if no tests or symptoms but might as well calculate

there is some idea of a kernel - each test or symptom will provide evidence 
around the sample date (before and after). so evidence or things nearby in time.

indirect evidence is the contacts. A contact with high risk informs a kernel as well
but this is directed in time, and only can influence events after the contact.


# Contact distributions

We have a weighted social network, consisting of by default a fixed number of
social contacts (when using the Watts-Strogatz generator), however in the future
this may be defined as a randomly distributed variable ($N$). The weights can be
interpreted as probabilities, and the number of social contacts as a sample
size. In the simulation contacts are selected based on Bernoulli sample using
the randomly assigned weight as a probability ($W$), adjusted by the two
partipants' baseline mobility ($M_1$, $M_2$) which is itself a probability (e.g.
that the person leaves home). The baseline mobility distributions could take any
form but for this example we show a Beta distribution. The challenge is 
to work out what distribution of weights ($W$) is needed to give the distribution of
contacts something that resembles reality.

$$
M_1 \sim Beta(95,5) \\
M_2 \sim Beta(95,5) \\
N \sim Point(300) \\
C \sim Binomial( N, W \times M_1 \times M_2) \\
$$

The number of contacts $C$ is dependent on the number of social contacts ($N$)
which sets an upper limit for the number of contacts. In the first instance we
consider the scenario where $N$ is a point distribution and $W$ is a uniform (our
initial modelling assumptions).

```{r}
# M1 and M2 are mobility baselines.

qkum = function(u, a, b) {
  return((1-(1-u)^(1/b))^(1/a))
}

rkum = function(n, a, b) {
  qkum(runif(n))
}

dkum = function(x, a, b) {
  return(
    a*b*(x^(a-1))*((1-x^a)^(b-1))
  )
}

ggplot()+
   geom_function(fun = ~dkum(.x, 0.02,0.4))


ggplot(tibble(x=qkum(seq(0,1,0.0001),1,6)), aes(x=x))+
   geom_histogram(binwidth = 0.01)


ggplot()+
   geom_function(fun = ~dkum(.x, 0.01,0.7))

ggplot(tibble(x=qkum(seq(0,1,0.0001),0.01,0.4)), aes(x=x))+
   geom_histogram(binwidth = 0.01)
```

```{r}

max_contacts = 100

pop = tibble(
  id = 1:1000,
  mob = #rbeta2(1000,0.5,0.25),
   runif(1000),
  or = rlnorm(1000,log(0.95),0.05),
  adj = expit(logit(mob)+log(or))
)

contacts = tibble(
  id1 = unlist(lapply(1:1000,rep,max_contacts)),
  off = rep(1:max_contacts,1000)
) %>% mutate(
  id2=(id1+off) %% 1000 + 1,
  w = #rbeta2(100000, 0.8,0.9)
    runif(1000 * max_contacts)
    # rkum(100000, 0.02,0.2)
    #rbeta(100000,0.1,0.3)# exp(1-1/runif(100000))
) %>% 
  inner_join(pop, by=join_by(id1==id)) %>%
  inner_join(pop, by=join_by(id2==id), suffix = c("1","2")) %>%
  glimpse()

pow = 0.75

base = contacts %>%
  filter(runif(1000 * max_contacts) < (w*mob1*mob2)) %>%
  group_by(id1) %>%
  count() %>%
  ungroup() %>%
  tidyr::complete(id1=as.integer(1:1000), fill = list(n=0))

adj = contacts %>%
  filter(runif(1000 * max_contacts) < (w*adj1*adj2)) %>%
  group_by(id1) %>%
  count() %>%
  ungroup() %>%
  tidyr::complete(id1=as.integer(1:1000), fill = list(n=0))

# base = contacts %>%
#   filter(runif(100000) < w) %>%
#   group_by(id1) %>%
#   count()
# 
# adj = contacts %>%
#   filter(runif(100000) < (expit(logit(w)+log(or1)+log(or2)))) %>%
#   group_by(id1) %>%
#   count()


ggplot(contacts, aes(x=w))+geom_density()




# dbeta2 = function(x, mu) {
#   sd = sqrt(pmin(
#     mu^2*(1-mu)/(1+mu),
#     (1-mu)^2*(mu)/(2-mu)
#   ))
#   a = (mu*(1-mu) / sd^2 -1) * mu
#   b = (mu*(1-mu) / sd^2 -1) * (1-mu)
#   return(dbeta(x,a,b))
# }
# 
# ggplot()+
#   lapply(seq(0.01,0.1,0.01), function(a) geom_function(fun = ~dbeta2(.x, a)))
# 
# 
# ggplot()+
#   geom_function(fun = ~dbeta(.x, 0.1,0.6))
# 
# ggplot()+
#   lapply(2:10, function(b) geom_function(fun = ~dbeta(.x, 0.2 , 0.2*b)))



ggplot()+
  geom_point(data=base,stat = "count",mapping = aes(x=n, y=after_stat(count)))+
  geom_point(data=adj,stat = "count",mapping = aes(x=n, y=after_stat(count)),colour="red")+
  scale_x_continuous(trans="log1p",limits = c(0,NA))+
  scale_y_continuous(trans="log1p")
```

# Barbarasi albert network

$$
P(k) \sim k^{-3} \\
P(k) = Ck^{-3} \\
C = \frac{1}{\sum_0^n{k^{-3}}} \\
\langle k \rangle =  C \sum_0^n k^{-2} = \frac{ \sum_0^n k^{-2} } {\sum_0^n k^{-3}} \\
\langle k^2 \rangle =  C \sum_0^n k^{-1} = \frac{ \sum_0^n k^{-1} } {\sum_0^n k^{-3}} \\
p_c = \frac{\langle k \rangle}{\langle k^2 \rangle - \langle k \rangle} \\
$$

```{r}



```

# Distrbiution of Euclidian distance on grid

```{r}

x=0:29
y=0:29

d=rep(0,(length(x)*length(y))^2)
i=1
for (x0 in x) {
  for (y0 in y) {
    for (x1 in x) {
      for (y1 in y) {
        if (x0 != x1 || y0 != y1) {
          d[i] = sqrt((x1-x0)^2+(y1-y0)^2)
          i = i+1
        }
      }
    }
  }
}

x0 = runif(100000)
x1 = runif(100000)
y0 = runif(100000)
y1 = runif(100000)
d = sqrt((x1-x0)^2+(y1-y0)^2)

ggplot(tibble(d=d),aes(x=d))+geom_density()

```
# Percolation threshold

* $\langle k \rangle$ mean of degree distribution
* $p_c$ percolation threshold ($=\lambda_c$ the effective infection rate - for SIR
model this is $\lambda = \frac{\beta}{\mu})$
* $\beta$ S to I transition rate
* $\mu$ I to R transition rate
* $\Lambda_1$ is largest eigenvalue of adjacency matrix
* $r_0$ is initial growth rate
* $R_0$ is initial reproduction number (>1)

$$
\lambda_c = p_c = \frac{\langle k \rangle}{\langle k^2 \rangle - \langle k \rangle} \\
\lambda_c = \frac{1}{\Lambda_1} \\
r_0 = \beta  \frac{1}{\lambda_c} \\
R_0 = 1 + \frac{r_0}{\mu} \\
R_0 = 1 + \frac{\lambda}{\lambda_c} \\
\lambda = \lambda_c(R_0-1) \\
p_{I} = 1-e^{-\lambda_c(R_0-1)}
$$


* $P(contact_{ij})$: The probability of contact on any given day
* $S_{ij}:$ The strength of a social network relationship between two individuals
$i$ (index case / infector) and $j$ (secondary case / infectee). A number from 0
(no probability of contact) to 1 (very close, e.g. family) in any given time
frame.
* $M_i(t)$: The mobility of an individual at any given number of days after index case infection
* $V_i(t)$: the viral load of index case $i$ as a function of time. This is an 
unbounded number greater than zero. A value of 1 is the threshold for 
infectiousness. Above this threshold the higher the viral load the more probable
the infection is.
* $c$: A normalising constant that defines the relationship between viral load
and transmission.


$$
P(contact_{ij}|t) = S_{ij} \times M_i(t) \times M_j(t) = k_{ij}\\

P(transmission_{i \rightarrow j}|contact_{ij},t) = 
     \begin{cases}
       0 &V_i(t)<1\\
       1-e^{-c(V_i(t)-1)} &V_i(t) \ge 1 \\
     \end{cases} \\
     
P(transmission_{i \rightarrow j}|t) = \begin{cases}
       0 &V_i(t)<1\\
       k_{ij} (1-e^{-c(V_i(t)-1)}) &V_i(t) \ge 1 \\
     \end{cases} \\
    
    

$$


The potential for transmission in this network is dependent on the expected 
viral load. We calculate a viral load profile for an average set of people infected
with an average infectious dose. The probability of transmission over all days is
the inverse probability that no transmission occurs on any day.

* $\omega_t = V_i(t)-1$: Expected infectious viral load at time t post infection 
where viral load is larger that zero.
* ${}^nC_k = \frac{n!}{k!(n-k)!}$

$$
\begin{align}
1- P(transmission_{i \rightarrow j}) &= \prod_t \bigg(1-k_{ij} (1-e^{-c\omega_t}) \bigg) \\
&= \prod_t \bigg((1-k_{ij}) + k_{ij}e^{-c\omega_t} \bigg) \\
&= \sum_{\tau=0}^t {}^tC_\tau(1-k_{ij})^{t-\tau} k_{ij}^te^{-c\sum_{s=0}^\tau \omega_s} \\
{}^tA_\tau = {}^tC_\tau(1-k_{ij})^{t-\tau} k_{ij}^t \\
\Omega_s = \sum_s{\omega_s} \\
1- P(transmission_{i \rightarrow j}) &= \sum_{\tau=0}^t {}^tA_\tau e^{-c\Omega_\tau} \\
1- P(transmission_{i \rightarrow j}) &= \sum_{\tau=0}^t {}^tA_\tau  \sum_{n=0}^\infty \frac{(-c\Omega_\tau)^n}{n!} \\
1- P(transmission_{i \rightarrow j}) &= \sum_{n=0}^\infty \sum_{\tau=0}^t   c^n \frac{ t! (1-k_{ij})^{t-\tau} k_{ij}^t  (-\Omega_\tau)^n}{\tau!(t-\tau)!n!} \\
\end{align}
$$






If we look at the average number of transmissions
across all possible infection states for the network, we imagine for each 
possible link ($ij$), what is the probability of transmission, if the index case
were infected at time 0. This is a transmission potential of the network.

Absolutely! Here's a **summary of the key equations** in **LaTeX form**, ready for you to copy and paste into your documentation, paper, or code comments.

---

## ðŸ§® 1. Precompute Viral Load Profile Coefficients

For each viral load profile $ m = 0,\dots,99 $, compute:

$$
\begin{aligned}
B_1^{(m)} &= \sum_{t: V^{(m)}(t) > 1} (V^{(m)}(t) - 1) \\
B_2^{(m)} &= \sum_{t: V^{(m)}(t) > 1} (V^{(m)}(t) - 1)^2 \\
B_3^{(m)} &= \sum_{t: V^{(m)}(t) > 1} (V^{(m)}(t) - 1)^3 \\
B_4^{(m)} &= \sum_{t: V^{(m)}(t) > 1} (V^{(m)}(t) - 1)^4
\end{aligned}
$$

---

## ðŸ“ˆ 2. Approximate Infectiousness for Profile $ m $

$$
A^{(m)}(c) \approx B_1^{(m)} c - \frac{B_2^{(m)}}{2} c^2 + \frac{B_3^{(m)}}{6} c^3 - \frac{B_4^{(m)}}{24} c^4
$$

---

## ðŸ”— 3. Compute Contact Strength

$$
k_{ij} = S_{ij} \cdot M_i \cdot M_j
$$

Where:
- $ S_{ij} $: social tie strength between individuals $ i $ and $ j $
- $ M_i, M_j $: mobility of individuals $ i $ and $ j $

---

## ðŸ“‰ 4. Taylor Approximation of Exponential

$$
e^{-k_{ij} A^{(m)}(c)} \approx 1 - x + \frac{x^2}{2} - \frac{x^3}{6} + \frac{x^4}{24}, \quad \text{where } x = k_{ij} A^{(m)}(c)
$$

---

## ðŸŽ¯ 5. Transmission Probability for Pair $ (i,j) $

$$
P(\text{transmission}_{i \rightarrow j} \mid c) = \frac{1}{100} \sum_{m=1}^{100} \left[1 - e^{-k_{ij} A^{(m)}(c)}\right]
$$

Using the polynomial approximation:

$$
P(\text{transmission}_{i \rightarrow j} \mid c) \approx \frac{1}{100} \sum_{m=1}^{100} \left[c_1^{(m)} c + c_2^{(m)} c^2 + c_3^{(m)} c^3 + c_4^{(m)} c^4 \right]
$$

Where the coefficients $ c_n^{(m)} $ are derived from $ k_{ij} $ and $ B_n^{(m)} $

---

## ðŸ“Š 6. Average Transmission Probability Over All Pairs

$$
\bar{P}(\text{transmission} \mid c) = \frac{1}{N_{ij}} \sum_{i,j} P(\text{transmission}_{i \rightarrow j} \mid c)
$$

Used to solve for $ c $ such that:

$$
\bar{P}(\text{transmission} \mid c) = R_0
$$

---

## âœ… Final Notes

These approximations allow you to:
- Avoid repeated calls to `exp()`
- Fully isolate $ c $
- Precompute all terms depending only on viral load profiles
- Evaluate transmission probability extremely fast during calibration or simulation

Let me know if you'd like these equations formatted as a PDF or embedded in Markdown!
